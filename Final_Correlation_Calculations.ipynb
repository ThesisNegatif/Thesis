{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Kevin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import weighted_sentiment_functions_final as wsff\n",
    "import stock_price_calculation_trials as spct\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do: organise the print statements, write code to save output lists into pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DELETE</th>\n",
       "      <th>CIK</th>\n",
       "      <th>TICKER</th>\n",
       "      <th>DATE</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0000002488</td>\n",
       "      <td>AMD</td>\n",
       "      <td>2019-02-08</td>\n",
       "      <td>Our 2018 financial results demonstrate the suc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0000002488</td>\n",
       "      <td>AMD</td>\n",
       "      <td>2018-02-27</td>\n",
       "      <td>2017 was an important year for AMD as we launc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0000002488</td>\n",
       "      <td>AMD</td>\n",
       "      <td>2017-02-21</td>\n",
       "      <td>As we continued to focus on our strategy to im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0000002488</td>\n",
       "      <td>AMD</td>\n",
       "      <td>2016-02-18</td>\n",
       "      <td>We faced a challenging business environment in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0000002488</td>\n",
       "      <td>AMD</td>\n",
       "      <td>2015-02-19</td>\n",
       "      <td>Net revenue for 2014 was $5.5 billion, an incr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0000766704</td>\n",
       "      <td>WELL</td>\n",
       "      <td>2009-03-02</td>\n",
       "      <td>Health Care REIT, Inc., an S&amp;P 500 company, is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0000766704</td>\n",
       "      <td>WELL</td>\n",
       "      <td>2008-02-28</td>\n",
       "      <td>Health Care REIT, Inc. is an equity real esta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0000766704</td>\n",
       "      <td>WELL</td>\n",
       "      <td>2007-03-01</td>\n",
       "      <td>Health Care REIT, Inc. is a self-administered,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0000766704</td>\n",
       "      <td>WELL</td>\n",
       "      <td>2006-03-10</td>\n",
       "      <td>Health Care REIT, Inc. is a self-administered,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0000766704</td>\n",
       "      <td>WELL</td>\n",
       "      <td>2005-03-16</td>\n",
       "      <td>Health Care REIT, Inc. is a self-administered,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>238 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     DELETE          CIK TICKER       DATE  \\\n",
       "0       NaN   0000002488    AMD 2019-02-08   \n",
       "1       NaN   0000002488    AMD 2018-02-27   \n",
       "2       NaN   0000002488    AMD 2017-02-21   \n",
       "3       NaN   0000002488    AMD 2016-02-18   \n",
       "4       NaN   0000002488    AMD 2015-02-19   \n",
       "..      ...          ...    ...        ...   \n",
       "233     NaN   0000766704   WELL 2009-03-02   \n",
       "234     NaN   0000766704   WELL 2008-02-28   \n",
       "235     NaN   0000766704   WELL 2007-03-01   \n",
       "236     NaN   0000766704   WELL 2006-03-10   \n",
       "237     NaN   0000766704   WELL 2005-03-16   \n",
       "\n",
       "                                                  TEXT  \n",
       "0    Our 2018 financial results demonstrate the suc...  \n",
       "1    2017 was an important year for AMD as we launc...  \n",
       "2    As we continued to focus on our strategy to im...  \n",
       "3    We faced a challenging business environment in...  \n",
       "4    Net revenue for 2014 was $5.5 billion, an incr...  \n",
       "..                                                 ...  \n",
       "233  Health Care REIT, Inc., an S&P 500 company, is...  \n",
       "234   Health Care REIT, Inc. is an equity real esta...  \n",
       "235  Health Care REIT, Inc. is a self-administered,...  \n",
       "236  Health Care REIT, Inc. is a self-administered,...  \n",
       "237  Health Care REIT, Inc. is a self-administered,...  \n",
       "\n",
       "[238 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = pd.read_csv('final_all_mdna_data.csv', sep='.@', header=None, names=['DELETE', 'CIK', 'TICKER', 'DATE', 'TEXT'], dtype={'CIK': object}, engine='python', parse_dates=[3], infer_datetime_format=True, encoding='utf-8')\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CIK</th>\n",
       "      <th>TICKER</th>\n",
       "      <th>DATE</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0000002488</td>\n",
       "      <td>AMD</td>\n",
       "      <td>2019-02-08</td>\n",
       "      <td>Our 2018 financial results demonstrate the suc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0000002488</td>\n",
       "      <td>AMD</td>\n",
       "      <td>2018-02-27</td>\n",
       "      <td>2017 was an important year for AMD as we launc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0000002488</td>\n",
       "      <td>AMD</td>\n",
       "      <td>2017-02-21</td>\n",
       "      <td>As we continued to focus on our strategy to im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0000002488</td>\n",
       "      <td>AMD</td>\n",
       "      <td>2016-02-18</td>\n",
       "      <td>We faced a challenging business environment in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0000002488</td>\n",
       "      <td>AMD</td>\n",
       "      <td>2015-02-19</td>\n",
       "      <td>Net revenue for 2014 was $5.5 billion, an incr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>0000766704</td>\n",
       "      <td>WELL</td>\n",
       "      <td>2009-03-02</td>\n",
       "      <td>Health Care REIT, Inc., an S&amp;P 500 company, is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234</td>\n",
       "      <td>0000766704</td>\n",
       "      <td>WELL</td>\n",
       "      <td>2008-02-28</td>\n",
       "      <td>Health Care REIT, Inc. is an equity real esta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>0000766704</td>\n",
       "      <td>WELL</td>\n",
       "      <td>2007-03-01</td>\n",
       "      <td>Health Care REIT, Inc. is a self-administered,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236</td>\n",
       "      <td>0000766704</td>\n",
       "      <td>WELL</td>\n",
       "      <td>2006-03-10</td>\n",
       "      <td>Health Care REIT, Inc. is a self-administered,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237</td>\n",
       "      <td>0000766704</td>\n",
       "      <td>WELL</td>\n",
       "      <td>2005-03-16</td>\n",
       "      <td>Health Care REIT, Inc. is a self-administered,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>238 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             CIK TICKER       DATE  \\\n",
       "0     0000002488    AMD 2019-02-08   \n",
       "1     0000002488    AMD 2018-02-27   \n",
       "2     0000002488    AMD 2017-02-21   \n",
       "3     0000002488    AMD 2016-02-18   \n",
       "4     0000002488    AMD 2015-02-19   \n",
       "..           ...    ...        ...   \n",
       "233   0000766704   WELL 2009-03-02   \n",
       "234   0000766704   WELL 2008-02-28   \n",
       "235   0000766704   WELL 2007-03-01   \n",
       "236   0000766704   WELL 2006-03-10   \n",
       "237   0000766704   WELL 2005-03-16   \n",
       "\n",
       "                                                  TEXT  \n",
       "0    Our 2018 financial results demonstrate the suc...  \n",
       "1    2017 was an important year for AMD as we launc...  \n",
       "2    As we continued to focus on our strategy to im...  \n",
       "3    We faced a challenging business environment in...  \n",
       "4    Net revenue for 2014 was $5.5 billion, an incr...  \n",
       "..                                                 ...  \n",
       "233  Health Care REIT, Inc., an S&P 500 company, is...  \n",
       "234   Health Care REIT, Inc. is an equity real esta...  \n",
       "235  Health Care REIT, Inc. is a self-administered,...  \n",
       "236  Health Care REIT, Inc. is a self-administered,...  \n",
       "237  Health Care REIT, Inc. is a self-administered,...  \n",
       "\n",
       "[238 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_all_data = all_data.drop(['DELETE'], axis=1)\n",
    "new_all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dataframe for any missing values:\n",
    "# new_all_data.isnull().values.any()\n",
    "# new_all_data.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0 420]\n",
      " [  1 667]\n",
      " [  2 887]]\n",
      "1.23657548126\n"
     ]
    }
   ],
   "source": [
    "# Comment/Uncomment following 4 lines to retrain the classifier\n",
    "trained_model, vocab_dict = wsff.train_classifier(classifier_model='Random_Forst')\n",
    "# save the model and vocab to disk\n",
    "filename = 'finalised_classifier_model_balanced.sav'\n",
    "pickle.dump(trained_model, open(filename, 'wb'))\n",
    "\n",
    "with open('vocab_mapping_dictionary.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the trained classifier model and vocab from disk\n",
    "filename = 'finalised_classifier_model_balanced.sav'\n",
    "model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "with open('vocab_mapping_dictionary.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To-do: Might need to modify this, instead round each value maybe?\n",
    "In regular sent analysis you get 0, 0.5, 1. With averaging justifications you get between 0 and 1, i.e. scaled/weighted.\n",
    "Below is more like weighted mode aggregated. Whereas above 'weighted' function is like weighted mean aggregated.\n",
    "But mode doesn't make much sense because it gets rid of the weight, so it is more like checking if 5 mined justifications can\n",
    "capture the full sentiment of the entire document.\n",
    "Read what you wrote in your thesis and then clarify all these sentiment score terms below!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Getting sentiment scores: 0Filings [00:00, ?Filings/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0 420]\n",
      " [  1 607]\n",
      " [  2 887]]\n",
      "1.24399164054\n",
      "token ids: [[366, 61, 233, 32, 1947, 225, 158, 61, 233, 225, 32, 5200], [366, 434, 278, 2583, 366, 1602, 877, 4355], [1082, 6362, 2911, 366, 434, 1268, 278, 364, 12, 5164, 1404, 765, 12, 203, 5949, 3997, 366, 434, 1853, 586, 4436, 2353, 1977, 1434, 434, 1853, 41, 44, 44, 5155, 5167, 12, 1208, 1434, 586, 2207, 4436, 610, 3554, 2890, 232, 2920, 366, 434, 1853], [4545, 1332, 256, 2022, 227, 228, 819, 119, 858, 254, 158, 284, 254, 227, 228, 220, 5200], [1082, 2192, 2520, 366, 3460, 3564, 86, 16, 2489, 366, 395, 500, 607, 696, 3564, 32, 1893]]\n",
      "X_test [[1082, 6362, 2911, 366, 434, 1268, 278, 364, 12, 5164, 1404, 765, 12, 203, 5949, 3997, 366, 434, 1853, 586, 4436, 2353, 1977, 1434, 434, 1853, 41, 44, 44, 5155, 5167, 12, 1208, 1434, 586, 2207, 4436, 610, 3554, 2890, 232, 2920, 366, 434, 1853], [4545, 1332, 256, 2022, 227, 228, 819, 119, 858, 254, 158, 284, 254, 227, 228, 220, 5200], [1082, 2192, 2520, 366, 3460, 3564, 86, 16, 2489, 366, 395, 500, 607, 696, 3564, 32, 1893]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Getting sentiment scores: 1Filings [00:05,  5.35s/Filings]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0 420]\n",
      " [  1 636]\n",
      " [  2 887]]\n",
      "1.24034997427\n",
      "token ids: [[1082, 345, 5892, 215, 3966, 366, 2204, 577, 2786, 229], [1082, 6362, 2911, 366, 434, 1268, 278, 364, 12, 5164, 1404, 765, 12, 203, 5949, 3997, 366, 434, 1853, 586, 4436, 2353, 1977, 1434, 434, 1853, 41, 44, 44, 5155, 5167, 12, 1208, 1434, 586, 2207, 4436, 610, 3554, 2890, 232, 2920, 366, 434, 1853], [17, 3564, 13, 97, 5894, 841, 336, 6042, 1534, 2813, 2874, 16, 4519, 5507, 32, 1898, 1319, 2533, 3964, 838], [1082, 2973, 366, 3460, 3564, 488, 5200, 2133, 2348, 1335, 1319, 6982, 393, 1177], [366, 61, 233, 32, 5200, 1947, 225, 158, 216, 61, 53, 225, 32, 5739]]\n",
      "X_test [[1082, 6362, 2911, 366, 434, 1268, 278, 364, 12, 5164, 1404, 765, 12, 203, 5949, 3997, 366, 434, 1853, 586, 4436, 2353, 1977, 1434, 434, 1853, 41, 44, 44, 5155, 5167, 12, 1208, 1434, 586, 2207, 4436, 610, 3554, 2890, 232, 2920, 366, 434, 1853], [17, 3564, 13, 97, 5894, 841, 336, 6042, 1534, 2813, 2874, 16, 4519, 5507, 32, 1898, 1319, 2533, 3964, 838]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Getting sentiment scores: 2Filings [00:08,  4.14s/Filings]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0 420]\n",
      " [  1 681]\n",
      " [  2 887]]\n",
      "1.23490945674\n",
      "token ids: [[357, 5739, 1082, 1202, 3093, 1628, 366, 61, 2543], [345, 889, 5739, 1082, 2973, 17, 1000, 770, 2923, 16, 216, 3569, 2166, 678, 6714, 1319, 32, 6982, 1512], [1082, 345, 4436, 366, 4098, 287, 4570, 32, 256, 813, 225, 5925, 3425, 3088, 309, 1249, 45, 5185, 4436, 4350], [1675, 2348, 366, 5894, 229, 632, 2133, 566, 1335, 2069, 336, 87], [61, 53, 32, 5739, 119, 225, 158, 225, 32]]\n",
      "X_test [[345, 889, 5739, 1082, 2973, 17, 1000, 770, 2923, 16, 216, 3569, 2166, 678, 6714, 1319, 32, 6982, 1512], [1082, 345, 4436, 366, 4098, 287, 4570, 32, 256, 813, 225, 5925, 3425, 3088, 309, 1249, 45, 5185, 4436, 4350]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Getting sentiment scores: 3Filings [00:11,  3.83s/Filings]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0 420]\n",
      " [  1 647]\n",
      " [  2 887]]\n",
      "1.23899692938\n",
      "token ids: [[13, 841, 215, 395, 500, 770, 80, 776, 5894, 32, 770], [1082, 345, 2520, 366, 2786, 1923, 57, 7164, 2133, 32, 607, 2069, 3474], [366, 61, 2543, 1908, 857, 254, 41, 1031, 254, 4511], [1082, 345, 1860, 25, 41, 366, 1826, 848, 740, 87, 3877, 2489, 1458, 2800, 291], [1082, 5677, 4473, 6, 278, 589, 1926, 2316, 61, 2543, 813, 930, 225, 5739]]\n",
      "X_test []\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-faadab862a53>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mweighted_sentiments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_all_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Getting sentiment scores'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Filings'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mweighted_sentiment\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwsff\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_weighted_sentiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'TEXT'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclustering_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Agglomerative'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mweighted_sentiments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweighted_sentiment\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweighted_sentiments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Final_Thesis_Files\\weighted_sentiment_functions_final.py\u001b[0m in \u001b[0;36mget_weighted_sentiment\u001b[1;34m(corpus, model, clustering_model)\u001b[0m\n\u001b[0;32m    441\u001b[0m     \u001b[0mjustifications\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJustificationMiner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclustering_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclustering_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_clusters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_X_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjustifications\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 443\u001b[1;33m     \u001b[0mjustifications_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    444\u001b[0m     \u001b[1;31m# print(justificationstwo)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[1;31m# print(justifications_predictions)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    536\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m         \"\"\"\n\u001b[1;32m--> 538\u001b[1;33m         \u001b[0mproba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    576\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'estimators_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m         \u001b[1;31m# Check data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 578\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    579\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m         \u001b[1;31m# Assign chunk of trees to jobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    355\u001b[0m                                  \"call `fit` before exploiting the model.\")\n\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    371\u001b[0m         \u001b[1;34m\"\"\"Validate X whenever one tries to predict, apply, predict_proba\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 373\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    374\u001b[0m             if issparse(X) and (X.indices.dtype != np.intc or\n\u001b[0;32m    375\u001b[0m                                 X.indptr.dtype != np.intc):\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    439\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m                     \u001b[1;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 441\u001b[1;33m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[0;32m    442\u001b[0m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m             \u001b[1;31m# To ensure that array flags are maintained\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "weighted_sentiments = []\n",
    "for i, row in tqdm(new_all_data.iterrows(), desc='Getting sentiment scores', unit='Filings'):\n",
    "    weighted_sentiment = wsff.get_weighted_sentiment(row['TEXT'], model, clustering_model='Agglomerative')\n",
    "    weighted_sentiments.append(weighted_sentiment)\n",
    "print(weighted_sentiments)\n",
    "new_all_data['weighted_sentiments'] = weighted_sentiments\n",
    "\n",
    "unweighted_sentiments = []\n",
    "for i, row in tqdm(new_all_data.iterrows(), desc='Getting sentiment scores', unit='Filings'):\n",
    "    unweighted_sentiment = wsff.get_unweighted_sentiment(row['TEXT'], model, clustering_model='Agglomerative')\n",
    "    unweighted_sentiments.append(unweighted_sentiment)\n",
    "print(unweighted_sentiments)\n",
    "new_all_data['unweighted_sentiments'] = unweighted_sentiments\n",
    "\n",
    "weighted_sentiments_DBSCAN = []\n",
    "for i, row in tqdm(new_all_data.iterrows(), desc='Getting sentiment scores', unit='Filings'):\n",
    "    weighted_sentiment = wsff.get_weighted_sentiment(row['TEXT'], model, clustering_model='DBSCAN')\n",
    "    weighted_sentiments_DBSCAN.append(weighted_sentiment)\n",
    "print(weighted_sentiments_DBSCAN)\n",
    "new_all_data['weighted_sentiments_DBSCAN'] = weighted_sentiments_DBSCAN\n",
    "\n",
    "\n",
    "weighted_sentiments_MeanShift = []\n",
    "for i, row in tqdm(new_all_data.iterrows(), desc='Getting sentiment scores', unit='Filings'):\n",
    "    weighted_sentiment = wsff.get_weighted_sentiment(row['TEXT'], model, clustering_model='MeanShift')\n",
    "    weighted_sentiments_MeanShift.append(weighted_sentiment)\n",
    "print(weighted_sentiments_MeanShift)\n",
    "new_all_data['weighted_sentiments_MeanShift'] = weighted_sentiments_MeanShift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_sentiments_mean = []\n",
    "\n",
    "for i, row in tqdm(new_all_data.iterrows(), desc='Getting sentiment scores', unit='Filings'):\n",
    "    full_sentiment_mean = wsff.get_full_sentiment_mean(row['TEXT'], model)\n",
    "    full_sentiments_mean.append(full_sentiment_mean)\n",
    "print(full_sentiments_mean)\n",
    "new_all_data['full_mean'] = full_sentiments_mean\n",
    "\n",
    "full_sentiments_mean_unrounded = []\n",
    "\n",
    "for i, row in tqdm(new_all_data.iterrows(), desc='Getting sentiment scores', unit='Filings'):\n",
    "    full_sentiment_mean_unrounded = wsff.get_full_sentiment_mean_unrounded(row['TEXT'], model)\n",
    "    full_sentiments_mean_unrounded.append(full_sentiment_mean_unrounded)\n",
    "print(full_sentiments_mean_unrounded)\n",
    "new_all_data['full_mean_unrounded'] = full_sentiments_mean_unrounded\n",
    "\n",
    "full_sentiments_mode = []\n",
    "\n",
    "for i, row in tqdm(new_all_data.iterrows(), desc='Getting sentiment scores', unit='Filings'):\n",
    "    full_sentiment_mode = wsff.get_full_sentiment_mode(row['TEXT'], model)\n",
    "    full_sentiments_mode.append(full_sentiment_mode)\n",
    "print(full_sentiments_mode)\n",
    "new_all_data['full_mode'] = full_sentiments_mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the time difference in weighted vs full sentiment scores:\n",
    "weighted:\n",
    "Getting sentiment scores: 239Filings [09:29,  2.38s/Filings]\n",
    "Getting sentiment scores: 239Filings [09:13,  2.32s/Filings]\n",
    "\n",
    "Full:\n",
    "Getting sentiment scores: 239Filings [00:04, 51.05Filings/s]\n",
    "Getting sentiment scores: 239Filings [00:04, 50.69Filings/s]\n",
    "Getting sentiment scores: 239Filings [00:04, 50.40Filings/s]\n",
    "\n",
    "Likely caused by the embeddings extraction time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trial outputs for all the below stock calculations:\n",
    "1:\n",
    "[23.049999237060547, 331.04998779296875, 26.155519485473633]\n",
    "[Timestamp('2019-02-08 00:00:00'), Timestamp('2019-01-31 00:00:00'), Timestamp('2010-11-18 00:00:00')]\n",
    "\n",
    "2:\n",
    "[52.259998321533203, 517.46002197265625, 16.585308074951172]\n",
    "[Timestamp('2020-02-10 00:00:00'), Timestamp('2020-01-31 00:00:00'), Timestamp('2011-11-18 00:00:00')]\n",
    "[27.090000152587891, 371.19000244140625, 27.930322647094727]\n",
    "[Timestamp('2019-05-08 00:00:00'), Timestamp('2019-04-30 00:00:00'), Timestamp('2011-02-18 00:00:00')]\n",
    "\n",
    "3:\n",
    "[126.72451215316249, 56.308727096605168, -36.589643787566928]\n",
    "[17.527119519516933, 12.125061509907129, 6.7855779450558877]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_stock_price_date(ticker, date):\n",
    "    given_date = date\n",
    "    while True:\n",
    "        try:\n",
    "            stock_price = spct.get_stock_adj_close(ticker, given_date)\n",
    "            break\n",
    "        except KeyError:\n",
    "            given_date = spct.add_days(given_date,1)\n",
    "    return given_date, stock_price\n",
    "\n",
    "stock_prices = []\n",
    "given_dates = []\n",
    "\n",
    "\n",
    "    \n",
    "for i, row in tqdm(new_all_data.iterrows(), desc='Getting stock prices', unit='Filings'):\n",
    "    given_date, stock_price = get_current_stock_price_date(row['TICKER'], row['DATE'])\n",
    "    stock_prices.append(stock_price)\n",
    "    given_dates.append(given_date)\n",
    "    \n",
    "print(stock_prices)\n",
    "print(given_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to make sure it isn't SBUX out of date 2019, index 209 should now return SBUX 2018-11-16\n",
    "print(new_all_data.iloc[209])\n",
    "print(given_dates[209])\n",
    "date_one_yearsbux = spct.add_years(given_dates[209], 1)\n",
    "print(date_one_yearsbux) # Should be 2019-11-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To-do: create 6 month window as well.\n",
    "# Getting an error at index 209: issue was SBUX 2019 is in Nov, so next year is Nov 2020, hasn't happened yet.\n",
    "stock_prices_next_year = []\n",
    "given_dates_next_year = []\n",
    "stock_prices_three_months = []\n",
    "given_dates_three_months = []\n",
    "\n",
    "for i, row in tqdm(new_all_data.iterrows(), desc='Getting stock prices', unit='Filings'):\n",
    "    date_one_year = spct.add_years(given_dates[i], 1)\n",
    "    given_date, stock_price = get_current_stock_price_date(row['TICKER'], date_one_year)\n",
    "    stock_prices_next_year.append(stock_price)\n",
    "    given_dates_next_year.append(given_date)\n",
    "    \n",
    "for i, row in tqdm(new_all_data.iterrows(), desc='Getting stock prices', unit='Filings'):\n",
    "    date_three_months = spct.add_months(given_dates[i], 3)\n",
    "    given_date, stock_price = get_current_stock_price_date(row['TICKER'], date_three_months)\n",
    "    stock_prices_three_months.append(stock_price)\n",
    "    given_dates_three_months.append(given_date)\n",
    "\n",
    "print(stock_prices_next_year)\n",
    "print(given_dates_next_year)\n",
    "print(stock_prices_three_months)\n",
    "print(given_dates_three_months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_changes_one_year = []\n",
    "percent_changes_three_months = []\n",
    "\n",
    "for old, new in zip(stock_prices, stock_prices_next_year):\n",
    "    change = spct.percent_change(old,new)\n",
    "    percent_changes_one_year.append(change)\n",
    "    \n",
    "for old, new in zip(stock_prices, stock_prices_three_months):\n",
    "    change = spct.percent_change(old,new)\n",
    "    percent_changes_three_months.append(change)\n",
    "\n",
    "new_all_data['% change 1 year'] = percent_changes_one_year\n",
    "new_all_data['% change 3 months'] = percent_changes_three_months\n",
    "\n",
    "print(percent_changes_one_year)\n",
    "print(percent_changes_three_months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_all_data.head()\n",
    "# To-do balance the training data to have less neutral labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_all_data.to_pickle(\"./full_data_df_1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name used in the functions below is still new_all_data, so maybe change the name below when reopening this file.\n",
    "new_all_data = pd.read_pickle(\"./full_data_df_1.pkl\")\n",
    "new_all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing spearmean versus pearson correlation:\n",
    "'I noted in a comment to the post that usually spearmanr() is used for ranks and it does not seem like you are correlating ranked data. I'm making that assessment based on non-integral values of the 2 sequences-usually ranks are non-negative integral values.\n",
    "\n",
    "Given the second issue (as it seems) I'd recommend using pearsonr(). Provided your version of scipy is 1.3.X or greater, the error message is informative and tells you exactly this issue:'\n",
    "\n",
    "Note: Set p-value to 0.05 or less to be statistically significant. \n",
    "'The null hypothesis is that the two variables are uncorrelated. The p-value is a number between zero and one that represents the probability that your data would have arisen if the null hypothesis were true.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block is for checking logit outputs on individual columns\n",
    "# from scipy import special, stats\n",
    "\n",
    "# normalised_sentiments_weighted = special.logit(new_all_data['full_mean_unrounded'])\n",
    "# normalised_sentiments_weighted_fixed = np.nan_to_num(normalised_sentiments_weighted)\n",
    "# # normalised_sentiments_weighted.values\n",
    "# normalised_sentiments_weighted_fixed\n",
    "# # np.isnan(normalised_sentiments_weighted_fixed).any()\n",
    "# # np.isinf(normalised_sentiments_weighted_fixed).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To-do: format printed lines with % stuff to make it more organised, google how to print with even spacing between\n",
    "# To-do: Reinclude MeanShift in the list below after restarting kernel with updated .py imports\n",
    "# To-do: Maybe calculate similarity scores between weighted sentiments lists and full sentiments lists\n",
    "# Using np.nan_to_num() to solve the infs error. Check the make sure the maths for this is okay\n",
    "from scipy import special, stats\n",
    "\n",
    "\n",
    "def logit_correlate(sentiments, percent_changes):\n",
    "    normalised_sentiments = special.logit(sentiments)\n",
    "#     print(normalised_sentiments.values)\n",
    "    normalised_sentiments_fixed = np.nan_to_num(normalised_sentiments)\n",
    "#     print(np.isnan(normalised_sentiments_fixed).any())\n",
    "#     print(np.isinf(normalised_sentiments_fixed).any())\n",
    "#     print(np.isnan(percent_changes.values).any())\n",
    "#     print(np.isinf(percent_changes.values).any())\n",
    "    r, p_value = stats.pearsonr(normalised_sentiments_fixed, percent_changes)\n",
    "    return r, p_value\n",
    "\n",
    "# for sentiment in ['weighted_sentiments', 'unweighted_sentiments', 'weighted_sentiments_DBSCAN', 'full_mean', 'full_mean_unrounded', 'full_mode']:\n",
    "#     print(sentiment)\n",
    "for sentiment in ['weighted_sentiments', 'unweighted_sentiments', 'weighted_sentiments_DBSCAN', 'weighted_sentiments_MeanShift', 'full_mean', 'full_mean_unrounded', 'full_mode']:\n",
    "    r_1yr, p_value_1yr = stats.pearsonr(new_all_data[sentiment], new_all_data['% change 1 year'])\n",
    "#     print(sentiment, '1 year', 'r=', r_1yr, 'p=', p_value_1yr)\n",
    "    time = '1 year'\n",
    "    print(f'{sentiment:30} {time:10} r={r_1yr:<25} p={p_value_1yr}')\n",
    "print('\\n')\n",
    "\n",
    "# for sentiment in ['weighted_sentiments', 'unweighted_sentiments', 'weighted_sentiments_DBSCAN', 'full_mean', 'full_mean_unrounded', 'full_mode']:\n",
    "for sentiment in ['weighted_sentiments', 'unweighted_sentiments', 'weighted_sentiments_DBSCAN', 'weighted_sentiments_MeanShift', 'full_mean', 'full_mean_unrounded', 'full_mode']:\n",
    "    r_3month, p_value_3month = stats.pearsonr(new_all_data[sentiment], new_all_data['% change 3 months'])\n",
    "#     print(sentiment, '3 months', 'r=', r_3month, 'p=', p_value_3month)\n",
    "    time = '3 months'\n",
    "    print(f'{sentiment:30} {time:10} r={r_3month:<25} p={p_value_3month}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results from correlation first attempt:\n",
    "\n",
    "# weighted_sentiments            1 year     r=-0.050994704451746364     p=0.4335826338188967\n",
    "# unweighted_sentiments          1 year     r=nan                       p=nan\n",
    "# weighted_sentiments_DBSCAN     1 year     r=-0.06924535039202072      p=0.2873655870349446\n",
    "# weighted_sentiments_MeanShift  1 year     r=-0.04549583883503607      p=0.4848381950662208\n",
    "# full_mean                      1 year     r=-0.07114038012496435      p=0.27434649762809193\n",
    "# full_mean_unrounded            1 year     r=-0.08882127181888153      p=0.1720160449195358\n",
    "# full_mode                      1 year     r=nan                       p=nan\n",
    "\n",
    "\n",
    "# weighted_sentiments            3 months   r=-0.07086475034518555      p=0.27621481315395385\n",
    "# unweighted_sentiments          3 months   r=nan                       p=nan\n",
    "# weighted_sentiments_DBSCAN     3 months   r=-0.04400432937353048      p=0.49928259331642083\n",
    "# weighted_sentiments_MeanShift  3 months   r=-0.003751647475437115     p=0.9540886896304036\n",
    "# full_mean                      3 months   r=-0.0534469143564483       p=0.4117689600688962\n",
    "# full_mean_unrounded            3 months   r=-0.12512998328588287      p=0.053876058043276866\n",
    "# full_mode                      3 months   r=nan                       p=nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To-do: maybe leave x axis unlabelled as it means nothing.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "t = new_all_data.index\n",
    "data1 = new_all_data['weighted_sentiments']\n",
    "data2 = new_all_data['% change 1 year']\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:purple'\n",
    "ax1.set_xlabel('index')\n",
    "ax1.set_ylabel('sentiment scores', color=color)\n",
    "ax1.plot(t, data1, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('% changes in stock price', color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot(t, data2, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = new_all_data.index\n",
    "data1 = new_all_data['weighted_sentiments_DBSCAN']\n",
    "data2 = new_all_data['% change 1 year']\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:purple'\n",
    "ax1.set_xlabel('index')\n",
    "ax1.set_ylabel('sentiment scores', color=color)\n",
    "ax1.plot(t, data1, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('% changes in stock price', color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot(t, data2, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
